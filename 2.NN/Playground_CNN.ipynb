{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beskrivelse\n",
    "Her gives det det samme kode som i Eksemple_CNN.ipynb. Modelen der kunne når et test accuracy på 95%. Her skal vi undersøg om vi kan lave modellen bedre, eller dårligere, og se hvordan at ændre de forskellige parametere har effekt på resultatet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Først importerer vi pakker og overfører datasætet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([1, 1, 28, 28])\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from CNN_utils.train import train\n",
    "from CNN_utils.test import test\n",
    "\n",
    "# Denne transform funktion gives til datasættet for at billederne kommer ud i den rigtig format, som er matricer med værdier mellem 0 og 1.\n",
    "# Vi normalisere pixlerne fra [0, 255] til [0, 1] fordi mest ML algoritmer er bygget til at arbejde bedst med normaliseret data.\n",
    "def image_transform(img):\n",
    "    return torchvision.transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# Overfører CIFAR10 træning og test datasætene fra pytorch\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=image_transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=image_transform)\n",
    "\n",
    "# Splitter træning sætet til en træning og validering sæt.\n",
    "# val_set_ratio bestemmer hvor meget af sættet bliver brugt til validering.\n",
    "val_set_ratio = 0.1\n",
    "train_set, val_set = random_split(train_set, [int(len(train_set)*(1-val_set_ratio)), int(len(train_set)*val_set_ratio)])\n",
    "\n",
    "# Tjekker størrelsen af billederne og hvor mange klasser der er\n",
    "print(\"Images shape:\", train_set[0][0].shape)\n",
    "print(\"Number of classes:\", len(np.unique(test_set.targets)))\n",
    "\n",
    "# Model / data parameter\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# Laver dataloadere der samler vores data i batches og shuffler dem hvis vi vil gerne\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Køre den initial model igen så du kan sammenlign med den senere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)  \n",
    "\n",
    "        # Vælg loss function og optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss\n",
    "        self.optimizer = optim.SGD\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Forudsig klasse\n",
    "        \n",
    "        Args:\n",
    "        x (np.ndarray): Input data\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Forudsiget klasse\n",
    "        \"\"\"\n",
    "        # Konverter til tensor\n",
    "        x = torch.Tensor(x).float()\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "\n",
    "        # Forudsig klasse\n",
    "        y_hat_prob = self(x)\n",
    "        y_hat = torch.argmax(y_hat_prob, dim=1)\n",
    "\n",
    "        return y_hat.detach().numpy()[0], y_hat_prob[0].detach().numpy()\n",
    "\n",
    "model = Net({\"epochs\": 15}, input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgaver: Undersøg forskellige modeller\n",
    "### Convolution lagene\n",
    "Nu skal du ændre parameter i modellen og se hvis de har en effekt på performance (val_accuracy) og træning hastighed (ms/step). Første prøv at ændre antal af convolution lag. Fjern den første convolution lag og se hvad der sker med validering accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)  \n",
    "\n",
    "        # Vælg loss function og optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss\n",
    "        self.optimizer = optim.SGD\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Forudsig klasse\n",
    "        \n",
    "        Args:\n",
    "        x (np.ndarray): Input data\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Forudsiget klasse\n",
    "        \"\"\"\n",
    "        # Konverter til tensor\n",
    "        x = torch.Tensor(x).float()\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "\n",
    "        # Forudsig klasse\n",
    "        y_hat_prob = self(x)\n",
    "        y_hat = torch.argmax(y_hat_prob, dim=1)\n",
    "\n",
    "        return y_hat.detach().numpy()[0], y_hat_prob[0].detach().numpy()\n",
    "\n",
    "model = Net({\"epochs\": 15}, input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu prøve at fjerne den anden convolution lag istedet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)  \n",
    "\n",
    "        # Vælg loss function og optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss\n",
    "        self.optimizer = optim.SGD\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Forudsig klasse\n",
    "        \n",
    "        Args:\n",
    "        x (np.ndarray): Input data\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Forudsiget klasse\n",
    "        \"\"\"\n",
    "        # Konverter til tensor\n",
    "        x = torch.Tensor(x).float()\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "\n",
    "        # Forudsig klasse\n",
    "        y_hat_prob = self(x)\n",
    "        y_hat = torch.argmax(y_hat_prob, dim=1)\n",
    "\n",
    "        return y_hat.detach().numpy()[0], y_hat_prob[0].detach().numpy()\n",
    "\n",
    "model = Net({\"epochs\": 15}, input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad hvis du sætter et ekstra convolution lag. Initialisere en ny Conv2D og sæt den efter MaxPooling2D. Du kan selv vælge parametrene af laget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)  \n",
    "\n",
    "        # Vælg loss function og optimizer\n",
    "        self.criterion = nn.CrossEntropyLoss\n",
    "        self.optimizer = optim.SGD\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Forudsig klasse\n",
    "        \n",
    "        Args:\n",
    "        x (np.ndarray): Input data\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Forudsiget klasse\n",
    "        \"\"\"\n",
    "        # Konverter til tensor\n",
    "        x = torch.Tensor(x).float()\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "\n",
    "        # Forudsig klasse\n",
    "        y_hat_prob = self(x)\n",
    "        y_hat = torch.argmax(y_hat_prob, dim=1)\n",
    "\n",
    "        return y_hat.detach().numpy()[0], y_hat_prob[0].detach().numpy()\n",
    "\n",
    "model = Net({\"epochs\": 15}, input_shape=input_shape, num_classes=num_classes)\n",
    "\n",
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign hastighed og accuracy forskel mellem de tre sidste modeller og den basal model. Var de bedre eller dårligere? Hurtigere eller langsomere? Hvorfor tror du det?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max pooling lagene\n",
    "Nu kigger vi på max pooling. Først prøv at fjern alle max pooling lagene og se hvad der sker med hastighed og accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu prøve at set et ekstre max pooling lag før den først convolution lag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad hvis vi bruger større kerner? Prøv kerne størrelse på (4,4) istedet for (2,2) i de to max pooling lage. Sådan at billederne bliver mindre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign de sidste tre modeler der bruger forskellige max pooling lag med den initial model. Hvorfor tror du der er forskel i træning hastighed og accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aktivering funktionet\n",
    "Aktivering funktionerne er brugt til at gør sikkert at modelen bliver ikke lineart, da en model uden aktivering kan kun finde lineart afhængigheder mellem dataen. Man kan alligevel skift aktivering funktion til en linear funktion ved at bruge strengen \"linear\" istedet for \"relu\". Træn en model der bruger kun linear aktivering i dens convolution lage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der findes også andre ikke lineart funktioner, prøv sigmoid og tanh aktivering funktionerne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sammenlign de forskellige aktiveringsfunktioner. Skal man altid bruge et non-lineart aktiveringsfunktion? Får man hurtigere træning ved at bruge lineart aktivering istedet for ikke lineart aktivering? Er de andre aktivering funktioner så god som RELU?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave: Undersøg træning parameter\n",
    "Det er ikke kun modellen der er vigtig for at få god resultater. Selve træning processen er også vigtigt.\n",
    "\n",
    "Epochen bestemer hvor langt modellen skal træn, for mange epocher kan resulterer i overfitting, og for færre epocher resulterer i underfitting.\n",
    "\n",
    "Batch størrelsen bestemmer hvor mange data punkter man bruge til at adjusterer vægtene med og hvor mange steps man tager per epoch som bestemmer hvor hurtigt vægtene ændres. Større batches resulterer i mere præcise vægt ændringer, men kan også resultarer i langsommere træning.\n",
    "\n",
    "Der også findes mange forskellige optimering algoritmer der opdaterer vægtene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antal Epocher\n",
    "Nu skal du undersøg om de førrig modeller underfitter. Prøv at øve antal epocher og se hvordan accuracy ændres. Pas på med at gå alt for stort, da flere epocher tager mere tid til at træne prøv at ikke gå over 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Et måde at finde en god epoch mængde er at starte med et stort epoch tal, og så se hvornår validering accuracy starter med at ændrer sig for lidt mellem hvert epoch (fx. 0.1). Dette strategi kaldes for early stopping, og den bruges for at gøre sikker at modellen træner ikke mere end den har bruge for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvad fandt du til at være en god mængde af epocher? Giver det mening at bruge det ekstra tid til at for den mængde øvede accuracy man får? Hvad hvis din model tog 10 minutter eller en time per epoch? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersøg træning af vores model\n",
    "Vi har gemt alt statistik fra vores træning under history variablen. Fra den kan vi hente udvikling af vores loss og accuracy igennem træning. Her skal du bruge matplotlib til at lave plotter til at vise udvikling af loss og accuracy. Prøve at se om modellen træner stabilt, eller om der er problemer som over og underfitting ved bruge af de her plotter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss af træning og validering data er givet, lave en matplotlib plot der viser dem:\n",
    "train_loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy af træning og validering data er givet, lave en matplotlib plot der viser dem:\n",
    "train_accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Andre optimering algoritmer\n",
    "Lige nu bruges der SGD som optimering algoritm. Der andre optimizers i keras som kan ses [her](https://keras.io/api/optimizers/) prøv at bruge adam istedet for SGD, som er nyere og er den mest brugt inden for ML lige nu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Var Adam bedre end SGD? Har Adam så mange Epochs som SGD? Brug din plot kode blokke igen for at se om Adam overfitter datasættet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss af træning og validering data er givet, lave en matplotlib plot der viser dem:\n",
    "train_loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy af træning og validering data er givet, lave en matplotlib plot der viser dem:\n",
    "train_accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opgave: En Sidste Model\n",
    "Nu har du prøvet mange forskellige modeller. Prøv at træne en model med de parametere du tror vil virke bedste og træn det. Når du tror at du har trænet den bedste model, så køre test kodeblokken og se hvor god din model kan detekterer hånd skrevet nummer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 5\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VENT!\n",
    "Husk at du skal kun bruge test datasætet en gang til sidste. Er du sikker at din nuværende model er den du vil gerne teste på?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hvor meget bedre er denne model end den initial model i Eksemple_CNN.ipynb?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
