{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neurale Netværk med Pytorch\n",
    "I den næste kode blok er der kodet et simpelt convolutional neurale netværk (CNN) til at genkende håndskreven cifre ved bruge af MNSIT datasætet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importering\n",
    "Her importerer vi de pakker vi skal bruge. Vi bruger:\n",
    "- numpy til nogle matematik og vektor operation\n",
    "- matplotlib til at vise billeder og plots\n",
    "- torch til at lave alt ML\n",
    "- Hjælpe træning og testing funktioner fra vores lokal sti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from CNN_utils.options import Hyperparameters, name_generator\n",
    "from CNN_utils.train import train\n",
    "from CNN_utils.test import test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasæt importering\n",
    "Heldigvis kan man overfør MNIST datasætet ved bruge af et funktion fra torch. Her overfører vi træning og testing datasætene, og så splitter vi træning sættet videre til træning og validereng sæt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne transform funktion gives til datasættet for at billederne kommer ud i den rigtig format, som er matricer med værdier mellem 0 og 1.\n",
    "# Vi normalisere pixlerne fra [0, 255] til [0, 1] fordi mest ML algoritmer er bygget til at arbejde bedst med normaliseret data.\n",
    "def image_transform(img):\n",
    "    return torchvision.transforms.ToTensor()(img).unsqueeze(0)\n",
    "\n",
    "# Overfører CIFAR10 træning og test datasætene fra pytorch\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=image_transform)\n",
    "test_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=image_transform)\n",
    "\n",
    "# Splitter træning sætet til en træning og validering sæt.\n",
    "# val_set_ratio bestemmer hvor meget af sættet bliver brugt til validering.\n",
    "val_set_ratio = 0.1\n",
    "train_set, val_set = random_split(train_set, [int(len(train_set)*(1-val_set_ratio)), int(len(train_set)*val_set_ratio)])\n",
    "\n",
    "# Tjekker størrelsen af billederne og hvor mange klasser der er\n",
    "print(\"Images shape:\", train_set[0][0].shape)\n",
    "print(\"Number of classes:\", len(np.unique(test_set.targets)))\n",
    "\n",
    "# Model / data parameter\n",
    "num_classes = 10\n",
    "input_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laver dataloadere der samler vores data i batches og shuffler dem hvis vi vil gerne\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi laver en løkke igennem datasætet og viser et billede fra hvert klasse. Vi kan finde hvad for noget klasser der er fra datasættets hjemmeside [her](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fig, ax = plt.subplots(2, 5, figsize=(20, 10))\n",
    "label_to_print = 0\n",
    "for imgs, labels in train_loader:\n",
    "    for img, label in zip(imgs, labels):\n",
    "        if label == label_to_print:\n",
    "            ax[label_to_print // 5, label_to_print % 5].imshow(img[0].permute(1,2,0), cmap=\"gray\")\n",
    "            ax[label_to_print // 5, label_to_print % 5].axis(\"off\")\n",
    "            ax[label_to_print // 5, label_to_print % 5].set_title(label_to_print)\n",
    "            label_to_print += 1\n",
    "        if label_to_print == num_classes:\n",
    "            break\n",
    "Fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialisering\n",
    "Derafter laver vi CNN modelen som vi skal træne til at genkende objekterne. Her specificerer vores model class som hedder Net. Der er der to vigtige metoder:\n",
    "\n",
    "Den første er \\_\\_init\\_\\_, som initialiserer modellen. Her initialiserer vi også alle de lag og moduler vi vil gerne bruge i modellen, som er:\n",
    "- nn.Conv2d, som er en convolutional lag. De fire argumenter vi giver den er: [Input kanaler, Output kanaler, størrelsen af kernen, antal steps kernen tager (stride)]\n",
    "- nn.Dropout, som tilfældigt vælger en procentdel af neuroneren og deaktiverer dem under træning. Denne model bruges til at undgå overfitting, og gør det at modellen lærer at bruge alle sine vægte. Den virker kun under træning og dens ene argument er procentdelen af neuroner der skal deaktiveres.\n",
    "- nn.Linear er et almideligt neural netværk lag, den tager som argumenter: [Antal input neuroner, Antal output neuroner].\n",
    "\n",
    "Den anden er forward, som præcis beskriver hvad der sker med billedene der kommer igennem netværket og i hvilket order. I vorse funktion sker dette:\n",
    "\n",
    "conv1 -> RELU -> conv2 -> RELU -> MaxPool2D -> dropout1 -> flatten -> fc1 -> RELU -> dropout2 -> fc2 -> softmax\n",
    "\n",
    "Hvor RELU og softmax er aktivering funktionerne, og flatten er en funktion der konverterer billederne til en vektor. Her bruger vi os MaxPool2D, men den blivet ikke initaliseret i \\_\\_init\\_\\_. Der er fordi MaxPool2D er en funktion, og ikke en lag der har vægter der skal trænes, så man behøver ikke initialiserer før man bruger den.\n",
    "\n",
    "Man kan nemt tilføje eller fjern lag og moduler fra netværket. Hvis den lag du vil gerne bruge har trænerbar vægter, så husk at initialiserer den under et variable før du bruger den under forward funktionen. Hvis du vil gerne bruge en funktion uden vægter, så kan du bare sætte den hvor den skal være i forward funktionen.\n",
    "\n",
    "Vores model har også 2 variable man kan ændre sidste i \\_\\_init\\_\\_ funktionen som ændrer træning processen. Man kan vælge loss funktion og optimering algoritme, lige nu bruges der CrossEntropyLoss, og SGD optimizer.\n",
    "\n",
    "De 3 argumenter under \\_\\_init\\_\\_ funktionen er dem man skal give når man først initialiserer modelen. Den første er en dictionary af hyperparameter der kan bruges under træning, disse kan være antal epochs, og optimering algoritmens learning rate. Et eksemple af model initialisering kan ses i den næste blok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"\n",
    "    Netværksarkitektur for klassifikation af billeder\n",
    "    \n",
    "    Args:\n",
    "    nn.Module: Superklasse for alle neurale netværk i PyTorch\n",
    "    \n",
    "    Returns:\n",
    "    Net: Netværksarkitektur\n",
    "    \"\"\"\n",
    "    def __init__(self, name, hyperparameters: dict = {}, input_shape = (1, 28, 28), num_classes: int = 3):\n",
    "        # Initialiserer architecturen\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Navngiv model\n",
    "        self.name = name\n",
    "\n",
    "        # Load Hyperparametre\n",
    "        self.hyperparameters = hyperparameters\n",
    "\n",
    "        # Vælg loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        setattr(self.hyperparameters, 'loss', self.criterion.__class__.__name__)\n",
    "\n",
    "        # Initialiserer model lag\n",
    "        self.input_shape = input_shape\n",
    "        self.conv1 = nn.Conv2d(input_shape[0], 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # Input størrelsen er følgende: (Antal output kanaler af conv2*((billedets højde minus 4 på grund af de 2 convolutions)/2 på grund af maxpool2D)*Det samme men med billedets bredde)\n",
    "        self.fc1 = nn.Linear(64*((self.input_shape[1]-4)//2)*((self.input_shape[2]-4)//2), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Forward pass af netværket\n",
    "        \n",
    "        Args:\n",
    "        x (torch.Tensor): Input tensor\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor\n",
    "        \"\"\"\n",
    "        x = x.reshape([-1] + list(self.input_shape))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her initialiserer vi modelen med en hyperparameter, vi siger at modelen skal træne for 5 epochs med en learning rate på 0.005 og Stochastic Gradient Descent (SGD) som optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sæt valgmuligheder\n",
    "hyperparameters = Hyperparameters(\n",
    "    lr = 0.005,\n",
    "    epochs = 5,\n",
    "    optimizer = optim.SGD,\n",
    ")\n",
    "\n",
    "# Hent model architecturene fra model_architecture.py\n",
    "model = Net(\n",
    "    name = name_generator(),\n",
    "    hyperparameters=hyperparameters, \n",
    "    input_shape=input_shape, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# tilføj optimizer til model\n",
    "model.optimizer = model.hyperparameters.optimizer(\n",
    "    model.parameters(),\n",
    "    lr=model.hyperparameters.lr,\n",
    "    momentum=model.hyperparameters.momentum,\n",
    ")\n",
    "setattr(model.hyperparameters, 'optimizer', model.optimizer.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Træning af modellen\n",
    "Før vi træner modellen er det en god ide at starte en mlflow server for at dokumentere hvordan vores model udvikler sig under træning. Det kan man gør ved at åbne en terminal og skrive:\n",
    "\n",
    "```\n",
    "mlflow server\n",
    "```\n",
    "\n",
    "ind i den. Luk ikke terminal, ellers slukker mlflow serveren. Du kan spørge en af arrangørene for hjælp med at sætte den op.\n",
    "\n",
    "Nu bruger vi træning funktionen fra hjælpe scripten. Den tage de to træning og validering dataloaders, og burger dem til at træne den model vi giver til den. Den kan tage lidt tid at være færdig med at træne modellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_loader, val_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing af Vores Model\n",
    "Til sidst bruger vi test funktionen fra hjælpe scriptet og de 10 tusind test billeder fra test datasætet til at evaluerer hvor god vores trænet model er til at klassifikerer billeder den har ikke set før."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opgaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentering opgaver findes i Playground_CNN.ipynb."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
